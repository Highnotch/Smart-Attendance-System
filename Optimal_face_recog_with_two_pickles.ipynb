{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0fa8b86-7aa0-4b23-8059-0caa4315f96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfbbab6-7712-4055-835d-bdff8e1ed581",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loading Pickle File For Know_Faces Data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2049ead0-18ab-41dc-a719-ac54e3244ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/junior_data_230.pkl\"\n",
    "face_pkl_230=open(filename,\"rb\")\n",
    "known_data_1=pkl.load(face_pkl_230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c3d638-0622-4b7d-aee9-2443a888fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/junior_data_remain.pkl\"\n",
    "face_pkl_remain=open(filename,\"rb\")\n",
    "known_data_2=pkl.load(face_pkl_remain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9516d3e8-3f01-4083-9423-501ff33d5690",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in known_data_2:\n",
    "    known_data_1.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f2b20d3-e6eb-4873-85a6-e13b0e5f11d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_data=known_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed4f0af8-1cb6-403e-8ad3-c3e628c3ccd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(known_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e878155f-981c-4e70-89e7-e011997e5d54",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Taking pictures from the camera  source and saving it in a directory .\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07c7fbc2-1aee-484a-9986-065a2d2e97cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "escape hit, closing the app\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "cam = cv2.VideoCapture(0)\n",
    "# making the image count as zero initially\n",
    "\n",
    "img_counter = 0\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print('failed to grab frame')\n",
    "        break\n",
    "    cv2.imshow('test', frame)\n",
    "    #to get continuous live video feed from my laptops webcam\n",
    "    k  = cv2.waitKey(1)\n",
    "    # Here the programe waits for the key to be pressed and return the key code \n",
    "    \n",
    "    # KEY CODE OF \"ESC\" is 27\n",
    "    \n",
    "    # KEY CODE OF \"Space\" is 32\n",
    "    \n",
    "    # SO , Therfore if the escape key is been pressed, the app will stop\n",
    "    \n",
    "    if k == 27:\n",
    "        print('escape hit, closing the app')\n",
    "        break\n",
    "    # if the spacebar key is been pressed\n",
    "    \n",
    "    # screenshots will be taken\n",
    "    \n",
    "    elif k  == 32:\n",
    "        \n",
    "        # the format for storing the images scrreenshotted\n",
    "        img_name =\"capture\"+str(img_counter)+\".jpg\"\n",
    "        loc=\"/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/\"+img_name\n",
    "        # saves the image as a png file\n",
    "        cv2.imwrite(loc, frame)\n",
    "        print('screenshot taken')\n",
    "        # the number of images automaticallly increases by 1\n",
    "        img_counter += 1\n",
    "\n",
    "# release the camera\n",
    "cam.release()\n",
    "\n",
    "# stops the camera window\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a87b796f-d78c-40f3-8432-47fb971d4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "pic_all=glob.glob(os.path.join(\"/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured\", '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c04bf3ac-100f-4970-a1d7-ac46ad6c27f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/test.jpg']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3f70527-aefb-4a8a-9331-aa5f05874893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def adjust_brightness_contrast(image, brightness=0, contrast=0):\n",
    "    # Apply brightness and contrast adjustments\n",
    "    adjusted_image = cv2.convertScaleAbs(image, alpha=contrast, beta=brightness)\n",
    "    return adjusted_image\n",
    "\n",
    "def check_dullness(image):\n",
    "    \n",
    "    # Calculating the average pixel intensity of the image\n",
    "    average_intensity = cv2.mean(image)[0]\n",
    "    \n",
    "    # Setting  a threshold for dullnessss\n",
    "    threshold = 200  \n",
    "    \n",
    "    if average_intensity < threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "count=0\n",
    "for image_path in pic_all:\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "# Checking if the image is dull or not!!!\n",
    "    if check_dullness(image):\n",
    "    # Adjust brightness and contrast\n",
    "        adjusted_image = adjust_brightness_contrast(image, brightness=10, contrast=1.1)\n",
    "    #cv2.imshow('Original Image', image)\n",
    "        cv2.imshow('Adjusted Image', adjusted_image)\n",
    "        cv2.imwrite(\"/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/\"+str(count)+\".jpg\",adjusted_image)\n",
    "        count=count+1\n",
    "    \n",
    "    #cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    else:\n",
    "        print(\"The image is not dull.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59ff038-435c-4dbc-99da-3c29d3cd5d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "\n",
    "# Load the input image\n",
    "image = cv2.imread('/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/test.jpg')\n",
    "\n",
    "# Detect face locations in the image\n",
    "face_locations = face_recognition.face_locations(image)\n",
    "\n",
    "# Adjust brightness and contrast for each detected face\n",
    "for face_location in face_locations:\n",
    "    top, right, bottom, left = face_location\n",
    "\n",
    "    # Extract the face region\n",
    "    face_image = image[top:bottom, left:right]\n",
    "\n",
    "    # Increase brightness and contrast of the face image\n",
    "    alpha = 1.2  # Brightness adjustment (1.0 means no change)\n",
    "    beta = 25  # Contrast adjustment (0 means no change)\n",
    "    adjusted_image = cv2.convertScaleAbs(face_image, alpha=alpha, beta=beta)\n",
    "\n",
    "    # Replace the original face region with the adjusted image\n",
    "    image[top:bottom, left:right] = adjusted_image\n",
    "\n",
    "# Display the modified image\n",
    "#cv2.imshow(\"Modified Image\", image)\n",
    "cv2.imwrite('/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Captured/test.jpg',image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4d9831fe-e662-4e07-80e1-92337035c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_names_1=os.listdir(\"/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/images_230\")\n",
    "#known_names_1.remove(\".DS_Store\")\n",
    "known_names_2=os.listdir(\"/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/images_remain\")\n",
    "#known_name_2.remove(\".DS_Store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5f1cd136-75c2-4d3f-aa42-c49b5e6778a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_names=known_names_1+(known_names_2)\n",
    "known_names.remove(\".DS_Store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d413b33e-9fdd-4555-b7d2-5cb00957452e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(known_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6a8d1934-be9e-442d-a61d-cd9d4260a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognised_faces=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f36342-b860-44d3-8609-643a26122dc7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### DRAWING A BOX AROUND FACES RECOGNISED IN THE PICTURE AND OUTPUTTING THE RECOGNISED FACES IN ALL THE IMAGES\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c3af9efc-0d6b-46d9-98e9-4681452c1e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to identify face locations 42.06313896179199\n",
      "time taken to encode faces 3.184621810913086\n",
      " \n",
      "time taken to identify face locations 41.507081031799316\n",
      "time taken to encode faces 3.1353800296783447\n",
      " \n",
      " \n",
      " \n",
      "['B22283.png', 'B22030.png', 'B22189.png', 'B22149.png', 'B22028.png', 'B22310.png', 'B22221.png', 'B22178.png', 'B22016.png', 'B22067.png', 'B22066.png', 'B22294.png', 'B22001.png', 'B22242.png', 'B22230.png', 'B22129.png', 'B22037.png', 'B22300.png', 'B22117.png', 'B22055.png', 'B22054.png', 'B22021.png', 'B22216.png', 'B22062.png', 'B22005.png', 'B22161.png']\n"
     ]
    }
   ],
   "source": [
    "img_counter=0\n",
    "for i in pic_all:\n",
    "    #starting counter \n",
    "    a=time.time()\n",
    "    img=cv2.imread(i)\n",
    "    \n",
    "    \n",
    "    face_locations = face_recognition.face_locations(img)\n",
    "    b=time.time()\n",
    "    print(\"time taken to identify face locations \"+ str(b-a))\n",
    "    \n",
    "    \n",
    "    face_encodings = face_recognition.face_encodings(img, face_locations,num_jitters=6)\n",
    "    c=time.time()\n",
    "    print(\"time taken to encode faces \"+str(c-b))\n",
    "    print(\" \")\n",
    "#changing here    img_counter=0\n",
    "    for face_encoding, face_location in zip(face_encodings, face_locations):\n",
    "        # Try to match the face with the known face\n",
    "        matches = face_recognition.compare_faces(known_data, face_encoding,tolerance=0.5)\n",
    "        #name = \"Unknown\"\n",
    "\n",
    "        # Find the best match\n",
    "        face_distances = face_recognition.face_distance(known_data, face_encoding)\n",
    "        best_match_index = np.argmin(face_distances)\n",
    "        if matches[best_match_index]:\n",
    "            name = known_names[best_match_index]\n",
    "            recognised_faces.append(name)\n",
    "            top, right, bottom, left = face_location\n",
    "            cv2.rectangle(img, (left+2, top+2), (right+2, bottom+2), (255, 0, 0), 4)\n",
    "            cv2.putText(img, name, (left + 6, bottom - 6), cv2.FONT_HERSHEY_SIMPLEX, 2.0, (0, 0, 255), 2)\n",
    "            cv2.imshow(\"out\",img)\n",
    "            img_name =\"capture\"+str(img_counter)+\".jpg\"\n",
    "            saved=\"/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Detected/\"+img_name\n",
    "            img_counter=img_counter+1\n",
    "            cv2.imwrite(saved,img)\n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "unique_faces=list(set(recognised_faces))\n",
    "#print(recognised_faces)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(unique_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c5776754-7f89-4b62-977c-bf478e2128af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(recognised_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3efeba1f-71c9-4892-86f3-2c199f6a8e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B22149.png': 2,\n",
       " 'B22055.png': 2,\n",
       " 'B22021.png': 2,\n",
       " 'B22216.png': 2,\n",
       " 'B22300.png': 2,\n",
       " 'B22230.png': 2,\n",
       " 'B22189.png': 2,\n",
       " 'B22030.png': 2,\n",
       " 'B22016.png': 2,\n",
       " 'B22066.png': 2,\n",
       " 'B22294.png': 2,\n",
       " 'B22242.png': 2,\n",
       " 'B22054.png': 2,\n",
       " 'B22221.png': 2,\n",
       " 'B22028.png': 2,\n",
       " 'B22310.png': 2,\n",
       " 'B22001.png': 2,\n",
       " 'B22037.png': 2,\n",
       " 'B22178.png': 2,\n",
       " 'B22117.png': 2,\n",
       " 'B22005.png': 2,\n",
       " 'B22161.png': 2,\n",
       " 'B22062.png': 2,\n",
       " 'B22283.png': 1,\n",
       " 'B22067.png': 1,\n",
       " 'B22129.png': 1}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = {}\n",
    "for items in recognised_faces:\n",
    "    freq[items] = recognised_faces.count(items)\n",
    "    \n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "10c86969-68b5-421a-99db-831f1bfa3415",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_att={}\n",
    "for i in freq.keys():\n",
    "    if (freq[i]>0.5*(len(pic_all))):\n",
    "        final_att[i]=freq[i]\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e6b36ce9-93d2-47fe-9b5f-eea7abaacd8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8e8d3989-461c-4a51-827c-0b56b95a7894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_attendance=list(final_att.keys())\n",
    "len(final_attendance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930a9787-d5d7-4d2a-bb12-b630507164e9",
   "metadata": {},
   "source": [
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "# DETECTING ALL FACES IN THE FRAME \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acc69707-8c84-44a4-b980-99c18167d67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to identify face locations 1.114548921585083\n",
      "time taken to encode faces 1.14182710647583\n",
      " \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for i in pic_all:\n",
    "    #starting counter \n",
    "    a=time.time()\n",
    "    img=cv2.imread(i)\n",
    "    \n",
    "    \n",
    "    face_locations = face_recognition.face_locations(img)\n",
    "    b=time.time()\n",
    "    print(\"time taken to identify face locations \"+ str(b-a))\n",
    "    \n",
    "    \n",
    "    face_encodings = face_recognition.face_encodings(img, face_locations,num_jitters=5)\n",
    "    c=time.time()\n",
    "    print(\"time taken to encode faces \"+str(c-b))\n",
    "    print(\" \")\n",
    "    img_counter=0\n",
    "    \n",
    "    \n",
    "    \n",
    "    for face_encoding, face_location in zip(face_encodings, face_locations):\n",
    "        # Try to match the face with the known faces\n",
    "        matches = face_recognition.compare_faces(known_data, face_encoding,tolerance=0.5)\n",
    "        #name = \"Unknown\"\n",
    "\n",
    "        # Find the best match\n",
    "        face_distances = face_recognition.face_distance(known_data, face_encoding)\n",
    "        best_match_index = np.argmin(face_distances)\n",
    "        \n",
    "        \n",
    "        #if matches[best_match_index]:\n",
    "        name = known_names[best_match_index]\n",
    "        \n",
    "        \n",
    "        recognised_faces.append(name)\n",
    "        top, right, bottom, left = face_location\n",
    "        cv2.rectangle(img, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "        #cv2.putText(img, name, (left + 6, bottom - 6), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 1)\n",
    "        \n",
    "        \n",
    "        cv2.imshow(\"out\",img)\n",
    "        img_name =\"capture\"+str(img_counter)+\".jpg\"\n",
    "        saved=\"/Users/sujith/Desktop/dp_project/Face-Recognition-Attendance-Projects-main/Detected/\"+img_name\n",
    "        img_counter=img_counter+1\n",
    "        cv2.imwrite(saved,img)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2821c746-1fc9-4ed8-ab98-0e262f102d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aman_min.jpg', 'bhunia-min.jpg', 'sujith-min.jpg']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognised_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53478d3d-1bc7-4c23-87ed-46bc93d5357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=known_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64670755-1829-4353-923b-8787c08f3a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5394694b-cb1d-4345-899f-0ade85d31d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f36aee5-192f-4b05-bce1-14206497a2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alekya.jpeg',\n",
       " 'simroop.jpeg',\n",
       " 'Bipin.jpeg',\n",
       " 'bhunia-min.jpg',\n",
       " 'Aditi.jpg',\n",
       " 'divyasree.jpeg',\n",
       " 'gargi.jpeg',\n",
       " 'Geetamsh.jpg',\n",
       " 'Nitya.jpeg',\n",
       " 'shravya.jpeg',\n",
       " 'nikhil.jpg',\n",
       " 'Yuraj.jpg',\n",
       " 'soham.jpg',\n",
       " 'sujith-min.jpg',\n",
       " 'Paras.jpg',\n",
       " 'anirudh.jpeg',\n",
       " 'Aritra.jpeg',\n",
       " 'Devidas.jpg',\n",
       " 'charan.jpeg',\n",
       " 'Anuj.jpeg',\n",
       " 'Sanjay.jpg',\n",
       " 'gaurav.jpeg',\n",
       " 'garima.jpeg',\n",
       " 'gopesh.jpeg',\n",
       " 'Mayank.jpeg',\n",
       " 'dhruv-min.jpg',\n",
       " 'aman_min.jpg',\n",
       " 'Piyush.jpg',\n",
       " 'jatin.jpeg']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1670703c-9bb7-4b7a-81bb-3696fc6650b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91458e82-9ce5-4fa9-8aab-eb30b2436bab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
